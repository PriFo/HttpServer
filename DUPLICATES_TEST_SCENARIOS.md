# –¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–µ–π

## –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 2025-01-20

---

## üéØ –¶–µ–ª—å

–°–æ–∑–¥–∞—Ç—å –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ "–ú–µ—Ç–æ–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–µ–π –≤ –ù–°–ò.md" –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.

---

## üìã –°—Ü–µ–Ω–∞—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–æ–¥—É

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞**: –ü—Ä–∞–≤–∏–ª–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–æ–ª—è–º (–∫–æ–¥, –∞—Ä—Ç–∏–∫—É–ª, GTIN)

**–¢–µ—Å—Ç**:

```go
func TestExactMatchByCode(t *testing.T) {
    analyzer := normalization.NewDuplicateAnalyzer()
    
    items := []normalization.DuplicateItem{
        {ID: 1, Code: "001", NormalizedName: "—Ç–æ–≤–∞—Ä 1"},
        {ID: 2, Code: "001", NormalizedName: "—Ç–æ–≤–∞—Ä 2"}, // –¢–æ—Ç –∂–µ –∫–æ–¥
        {ID: 3, Code: "002", NormalizedName: "—Ç–æ–≤–∞—Ä 1"}, // –î—Ä—É–≥–æ–π –∫–æ–¥
    }
    
    groups := analyzer.AnalyzeDuplicates(items)
    
    // –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω–∞ –≥—Ä—É–ø–ø–∞ —Å ID 1 –∏ 2 (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∫–æ–¥)
    found := false
    for _, group := range groups {
        if group.Type == normalization.DuplicateTypeExact {
            ids := make(map[int]bool)
            for _, item := range group.Items {
                ids[item.ID] = true
            }
            if ids[1] && ids[2] && !ids[3] {
                found = true
                if group.SimilarityScore != 1.0 {
                    t.Errorf("Exact match –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å similarity 1.0, –ø–æ–ª—É—á–∏–ª–∏ %.2f", 
                        group.SimilarityScore)
                }
                break
            }
        }
    }
    
    if !found {
        t.Error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã exact duplicates –ø–æ –∫–æ–¥—É")
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –ì—Ä—É–ø–ø–∞ —Å ID 1 –∏ 2, similarity = 1.0

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ ‚â§ 2

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 663): "–µ—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ ‚â§ 2, —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–æ—Ö–æ–∂–∏–º–∏"

**–¢–µ—Å—Ç**:

```go
func TestLevenshteinThreshold(t *testing.T) {
    import "httpserver/normalization/algorithms"
    
    testCases := []struct {
        s1, s2    string
        threshold int
        expected  bool
    }{
        {"–¥–æ–º", "—Ç–æ–º", 1, true},      // –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ 1
        {"–∫–æ—Ç", "–∫–æ—Ç–µ–Ω–æ–∫", 3, true},  // –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ 3
        {"–∫–æ—Ç", "–∫–æ—Ç–µ–Ω–æ–∫", 2, false}, // –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ 3 > 2
        {"abc", "def", 2, false},      // –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ 3
    }
    
    for _, tc := range testCases {
        distance := algorithms.LevenshteinDistance(tc.s1, tc.s2)
        similarity := algorithms.LevenshteinSimilarity(tc.s1, tc.s2)
        
        isSimilar := distance <= tc.threshold
        
        if isSimilar != tc.expected {
            t.Errorf("Levenshtein(%q, %q): distance=%d, threshold=%d, expected=%v, got=%v",
                tc.s1, tc.s2, distance, tc.threshold, tc.expected, isSimilar)
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ similarity –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
        if similarity < 0 || similarity > 1 {
            t.Errorf("Similarity –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1], –ø–æ–ª—É—á–∏–ª–∏ %.2f", similarity)
        }
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: N-–≥—Ä–∞–º–º—ã —Å —Ñ–æ—Ä–º—É–ª–æ–π –°—ë—Ä–µ–Ω—Å–µ–Ω–∞

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 687-700): –§–æ—Ä–º—É–ª–∞ –°—ë—Ä–µ–Ω—Å–µ–Ω–∞ –¥–ª—è N-–≥—Ä–∞–º–º

**–¢–µ—Å—Ç**:

```go
func TestNGramSorensenFormula(t *testing.T) {
    gen := algorithms.NewNGramGenerator(2)
    
    // –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: "–∑–≤–µ–Ω–æ" vs "–∑–µ—Ä–Ω–æ"
    s1 := "–∑–≤–µ–Ω–æ"
    s2 := "–∑–µ—Ä–Ω–æ"
    
    ngrams1 := gen.Generate(s1)
    ngrams2 := gen.Generate(s2)
    
    // –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    intersection := 0
    ngrams1Set := make(map[string]bool)
    for _, n := range ngrams1 {
        ngrams1Set[n] = true
    }
    
    for _, n := range ngrams2 {
        if ngrams1Set[n] {
            intersection++
        }
    }
    
    // –§–æ—Ä–º—É–ª–∞ –°—ë—Ä–µ–Ω—Å–µ–Ω–∞: 2 √ó |A ‚à© B| / (|A| + |B|)
    union := len(ngrams1) + len(ngrams2) - intersection
    sorensen := 2.0 * float64(intersection) / float64(union)
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
    similarity := gen.Similarity(s1, s2)
    
    // –î–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –∏–∑-–∑–∞ padding
    if math.Abs(similarity-sorensen) > 0.1 {
        t.Errorf("–°—Ö–æ–∂–µ—Å—Ç—å –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º—É–ª–µ –°—ë—Ä–µ–Ω—Å–µ–Ω–∞: –æ–∂–∏–¥–∞–ª–∏ ~%.2f, –ø–æ–ª—É—á–∏–ª–∏ %.2f",
            sorensen, similarity)
    }
    
    // –ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: –æ–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ~0.25
    if similarity < 0.2 || similarity > 0.3 {
        t.Logf("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Å—Ö–æ–∂–µ—Å—Ç—å %.2f –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π 0.25", similarity)
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ Similarity ‚âà 0.25 (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 4: –ú–µ—Ç—Ä–∏–∫–∏ Precision –∏ Recall

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 45-47, 87-89): –§–æ—Ä–º—É–ª—ã Precision –∏ Recall

**–¢–µ—Å—Ç**:

```go
func TestPrecisionRecallFormulas(t *testing.T) {
    metrics := normalization.NewEvaluationMetrics()
    
    // –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    testCases := []struct {
        name                string
        tp, fp, fn, tn      int
        expectedPrecision   float64
        expectedRecall      float64
        expectedF1          float64
    }{
        {
            name:              "–ò–¥–µ–∞–ª—å–Ω—ã–π —Å–ª—É—á–∞–π",
            tp:                100,
            fp:                0,
            fn:                0,
            tn:                900,
            expectedPrecision: 1.0,
            expectedRecall:    1.0,
            expectedF1:         1.0,
        },
        {
            name:              "–° –ª–æ–∂–Ω—ã–º–∏ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è–º–∏",
            tp:                90,
            fp:                10,
            fn:                5,
            tn:                895,
            expectedPrecision: 0.9,  // 90 / (90 + 10)
            expectedRecall:    0.947, // 90 / (90 + 5) ‚âà 0.947
            expectedF1:         0.923, // 2 √ó (0.9 √ó 0.947) / (0.9 + 0.947)
        },
    }
    
    for _, tc := range testCases {
        t.Run(tc.name, func(t *testing.T) {
            matrix := normalization.ConfusionMatrix{
                TruePositive:  tc.tp,
                FalsePositive: tc.fp,
                FalseNegative: tc.fn,
                TrueNegative:  tc.tn,
            }
            
            result := metrics.CalculateMetrics(matrix)
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º Precision: TP / (TP + FP)
            if math.Abs(result.Precision-tc.expectedPrecision) > 0.01 {
                t.Errorf("Precision: –æ–∂–∏–¥–∞–ª–∏ %.3f, –ø–æ–ª—É—á–∏–ª–∏ %.3f",
                    tc.expectedPrecision, result.Precision)
            }
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º Recall: TP / (TP + FN)
            if math.Abs(result.Recall-tc.expectedRecall) > 0.01 {
                t.Errorf("Recall: –æ–∂–∏–¥–∞–ª–∏ %.3f, –ø–æ–ª—É—á–∏–ª–∏ %.3f",
                    tc.expectedRecall, result.Recall)
            }
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º F1: 2 √ó (P √ó R) / (P + R)
            if math.Abs(result.F1Score-tc.expectedF1) > 0.01 {
                t.Errorf("F1: –æ–∂–∏–¥–∞–ª–∏ %.3f, –ø–æ–ª—É—á–∏–ª–∏ %.3f",
                    tc.expectedF1, result.F1Score)
            }
        })
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –í—Å–µ —Ñ–æ—Ä–º—É–ª—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 5: –û—à–∏–±–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ —Ä–æ–¥–∞

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 48-49):
- –û—à–∏–±–∫–∏ –ø–µ—Ä–≤–æ–≥–æ —Ä–æ–¥–∞ (–ª–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞) –Ω–µ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–≤—ã—à–∞—Ç—å 10%
- –û—à–∏–±–∫–∏ –≤—Ç–æ—Ä–æ–≥–æ —Ä–æ–¥–∞ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –±–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å) –Ω–µ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–≤—ã—à–∞—Ç—å 5%

**–¢–µ—Å—Ç**:

```go
func TestErrorRates(t *testing.T) {
    metrics := normalization.NewEvaluationMetrics()
    
    // –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å –¥–æ–ø—É—Å—Ç–∏–º—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏
    matrix := normalization.ConfusionMatrix{
        TruePositive:  90,
        FalsePositive: 10, // 10% –æ—Ç –≤—Å–µ—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
        FalseNegative: 5,  // 5% –æ—Ç –≤—Å–µ—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
        TrueNegative:  895,
    }
    
    result := metrics.CalculateMetrics(matrix)
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    maxFPR := 0.10 // 10%
    maxFNR := 0.05 // 5%
    
    if result.FalsePositiveRate > maxFPR {
        t.Errorf("FPR (%.2f%%) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø–æ—Ä–æ–≥ (%.2f%%)",
            result.FalsePositiveRate*100, maxFPR*100)
    }
    
    if result.FalseNegativeRate > maxFNR {
        t.Errorf("FNR (%.2f%%) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø–æ—Ä–æ–≥ (%.2f%%)",
            result.FalseNegativeRate*100, maxFNR*100)
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é
    requirements := normalization.DefaultQualityRequirements()
    requirements.MaxFalsePositiveRate = 0.10
    requirements.MaxFalseNegativeRate = 0.05
    
    validation := metrics.ValidateMetrics(result, requirements)
    
    if !validation.MeetsRequirements {
        t.Errorf("–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º: %v", validation.Violations)
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ FPR ‚â§ 10%, FNR ‚â§ 5%

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 6: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 600-623): –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–π –æ—á–∏—Å—Ç–∫–∏

**–¢–µ—Å—Ç**:

```go
func TestTextPreprocessing(t *testing.T) {
    normalizer := normalization.NewNameNormalizer()
    
    testCases := []struct {
        input    string
        expected string
        desc     string
    }{
        {
            input:    "–ú–ê–°–õ–û –°–õ–ò–í–û–ß–ù–û–ï",
            expected: "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ",
            desc:     "–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É",
        },
        {
            input:    "  –º–∞—Å–ª–æ   —Å–ª–∏–≤–æ—á–Ω–æ–µ  ",
            expected: "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ",
            desc:     "–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤",
        },
        {
            input:    "–ú–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ, 82%",
            expected: "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ",
            desc:     "–£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —á–∏—Å–µ–ª",
        },
        {
            input:    "WBC00Z0002 –ö–∞–±–µ–ª—å –í–í–ì 3x2.5 120mm",
            expected: "–∫–∞–±–µ–ª—å –≤–≤–≥",
            desc:     "–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–¥–æ–≤, —Ä–∞–∑–º–µ—Ä–æ–≤, –µ–¥–∏–Ω–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏—è",
        },
    }
    
    for _, tc := range testCases {
        t.Run(tc.desc, func(t *testing.T) {
            result := normalizer.NormalizeName(tc.input)
            if result != tc.expected {
                t.Errorf("–û–∂–∏–¥–∞–ª–∏ '%s', –ø–æ–ª—É—á–∏–ª–∏ '%s'", tc.expected, result)
            }
        })
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 7: –°—Ç–µ–º–º–∏–Ω–≥

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 630-632): "–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è" + "–Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω" ‚Üí "–Ω–æ—Ä–º–∞–ª–∏–∑"

**–¢–µ—Å—Ç**:

```go
func TestStemming(t *testing.T) {
    stemmer := algorithms.NewRussianStemmer()
    
    testCases := []struct {
        word     string
        expected string
    }{
        {"–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è", "–Ω–æ—Ä–º–∞–ª–∏–∑"},
        {"–Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω", "–Ω–æ—Ä–º–∞–ª–∏–∑"},
        {"–º–∞—Å–ª–∞–º–∏", "–º–∞—Å–ª"}, // –°—Ç–µ–º–º–∏–Ω–≥ (–Ω–µ –ø–æ–ª–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)
    }
    
    for _, tc := range testCases {
        result := stemmer.Stem(tc.word)
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ—Ä–Ω–∏ –ø–æ—Ö–æ–∂–∏ (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã)
        if len(result) == 0 {
            t.Errorf("–°—Ç–µ–º–º–∏–Ω–≥ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è '%s'", tc.word)
        }
        // –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        t.Logf("–°–ª–æ–≤–æ: %s ‚Üí –°—Ç–µ–º: %s", tc.word, result)
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ "–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è" –∏ "–Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω" –¥–∞—é—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–æ—Ä–Ω–∏
    stem1 := stemmer.Stem("–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è")
    stem2 := stemmer.Stem("–Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ 6 —Å–∏–º–≤–æ–ª–æ–≤)
    if len(stem1) >= 6 && len(stem2) >= 6 {
        if stem1[:6] != stem2[:6] {
            t.Errorf("–ö–æ—Ä–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—Ö–æ–∂–∏: '%s' vs '%s'", stem1, stem2)
        }
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –°—Ç–µ–º–º–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫–æ—Ä–Ω–∏ –ø–æ—Ö–æ–∂–∏

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 8: –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 702-706): –ü–æ–∏—Å–∫ –¥—É–±–ª–µ–π –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã

**–¢–µ—Å—Ç**:

```go
func TestPhoneticAlgorithms(t *testing.T) {
    soundex := algorithms.NewSoundexRU()
    metaphone := algorithms.NewMetaphoneRU()
    
    // –¢–µ—Å—Ç–æ–≤—ã–µ –ø–∞—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ—Ö–æ–∂–∏ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏
    testPairs := []struct {
        s1, s2   string
        expected bool
    }{
        {"–ò–≤–∞–Ω–æ–≤", "Ivanov", true},  // –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ vs –ª–∞—Ç–∏–Ω–∏—Ü–∞
        {"–º–æ–ª–æ—Ç–æ–∫", "–º–æ–ª–æ—Ç–∞–∫", true}, // –û–ø–µ—á–∞—Ç–∫–∞
    }
    
    for _, pair := range testPairs {
        soundexSim := soundex.Similarity(pair.s1, pair.s2)
        metaphoneSim := metaphone.Similarity(pair.s1, pair.s2)
        
        if pair.expected {
            if soundexSim < 0.5 && metaphoneSim < 0.5 {
                t.Logf("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–∏–∑–∫–∞—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å –¥–ª—è '%s' vs '%s': Soundex=%.2f, Metaphone=%.2f",
                    pair.s1, pair.s2, soundexSim, metaphoneSim)
            }
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–¥—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è
        code1 := soundex.Encode(pair.s1)
        code2 := soundex.Encode(pair.s2)
        
        if code1 == "" || code2 == "" {
            t.Errorf("Soundex –∫–æ–¥ –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º –¥–ª—è '%s' –∏–ª–∏ '%s'", pair.s1, pair.s2)
        }
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 9: –í—ã–±–æ—Ä –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 796-798): –í—ã–±–æ—Ä –ø–æ –ø–æ–ª–Ω–æ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

**–¢–µ—Å—Ç**:

```go
func TestMasterRecordSelection(t *testing.T) {
    analyzer := normalization.NewDuplicateAnalyzer()
    
    items := []normalization.DuplicateItem{
        {
            ID:             1,
            NormalizedName: "–∫–∞–±–µ–ª—å",
            QualityScore:   0.7,
            MergedCount:    0,
            ProcessingLevel: "basic",
        },
        {
            ID:             2,
            NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5 120mm –º–µ–¥–Ω—ã–π",
            QualityScore:   0.95,
            MergedCount:    2,
            ProcessingLevel: "benchmark",
        },
        {
            ID:             3,
            NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥",
            QualityScore:   0.8,
            MergedCount:    1,
            ProcessingLevel: "ai_enhanced",
        },
    }
    
    masterID := analyzer.selectMasterRecord(items)
    
    // –ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å ID 2 (–Ω–∞–∏–±–æ–ª—å—à–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
    if masterID != 2 {
        t.Errorf("–û–∂–∏–¥–∞–ª–∏ –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å ID=2, –ø–æ–ª—É—á–∏–ª–∏ ID=%d", masterID)
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–º–µ–µ—Ç –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    var master normalization.DuplicateItem
    for _, item := range items {
        if item.ID == masterID {
            master = item
            break
        }
    }
    
    if len(master.NormalizedName) < len(items[0].NormalizedName) {
        t.Error("–ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –í—ã–±–∏—Ä–∞–µ—Ç—Å—è –∑–∞–ø–∏—Å—å —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –ø–æ–ª–Ω–æ—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 10: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥—É–±–ª–µ–π

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞** (—Å—Ç—Ä–æ–∫–∞ 805-813): –ü–æ–ª–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä

**–¢–µ—Å—Ç**:

```go
func TestFullPipeline(t *testing.T) {
    nsi := normalization.NewNSINormalizer()
    
    // –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥—É–±–ª–µ–π
    items := []normalization.DuplicateItem{
        // Exact duplicates
        {ID: 1, Code: "001", NormalizedName: "–º–æ–ª–æ—Ç–æ–∫"},
        {ID: 2, Code: "001", NormalizedName: "–º–æ–ª–æ—Ç–æ–∫"},
        
        // Fuzzy duplicates (–æ–ø–µ—á–∞—Ç–∫–∏)
        {ID: 3, Code: "002", NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
        {ID: 4, Code: "003", NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
        
        // Semantic duplicates
        {ID: 5, Code: "004", NormalizedName: "–∫–∞–±–µ–ª—å –º–µ–¥–Ω—ã–π"},
        {ID: 6, Code: "005", NormalizedName: "–∫–∞–±–µ–ª—å –∏–∑ –º–µ–¥–∏"},
        
        // Word-based duplicates
        {ID: 7, Code: "006", NormalizedName: "–∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π"},
        {ID: 8, Code: "007", NormalizedName: "–∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π –ø–æ–ª–Ω–æ—Ç–µ–ª—ã–π"},
    }
    
    config := normalization.DefaultDuplicateDetectionConfig()
    config.UseExactMatching = true
    config.UseFuzzyMatching = true
    config.Threshold = 0.85
    config.MergeOverlapping = true
    
    groups := nsi.FindDuplicates(items, config)
    
    // –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω—ã –≥—Ä—É–ø–ø—ã:
    // 1. Exact: ID 1, 2
    // 2. Fuzzy: ID 3, 4
    // 3. Semantic: ID 5, 6 (–≤–æ–∑–º–æ–∂–Ω–æ)
    // 4. Word-based: ID 7, 8
    
    if len(groups) == 0 {
        t.Error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞–π–¥–µ–Ω—ã exact duplicates
    foundExact := false
    for _, group := range groups {
        if group.Type == normalization.DuplicateTypeExact {
            ids := make(map[int]bool)
            for _, item := range group.Items {
                ids[item.ID] = true
            }
            if ids[1] && ids[2] {
                foundExact = true
                break
            }
        }
    }
    
    if !foundExact {
        t.Error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã exact duplicates")
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –µ—Å—Ç—å –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å
    for _, group := range groups {
        if group.SuggestedMaster == 0 {
            t.Errorf("–ì—Ä—É–ø–ø–∞ %s –Ω–µ –∏–º–µ–µ—Ç –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏", group.GroupID)
        }
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –í—Å–µ —Ç–∏–ø—ã –¥—É–±–ª–µ–π –Ω–∞–π–¥–µ–Ω—ã, –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏ –≤—ã–±—Ä–∞–Ω—ã

---

## üìä –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞

### –ß–µ–∫-–ª–∏—Å—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–æ–∫—É–º–µ–Ω—Ç—É

- [x] –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ —Å –ø–æ—Ä–æ–≥–æ–º ‚â§ 2
- [x] N-–≥—Ä–∞–º–º—ã —Å —Ñ–æ—Ä–º—É–ª–æ–π –°—ë—Ä–µ–Ω—Å–µ–Ω–∞
- [x] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—Ä–µ–≥–∏—Å—Ç—Ä, –ø—Ä–æ–±–µ–ª—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è)
- [x] –°—Ç–µ–º–º–∏–Ω–≥ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- [x] –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã (Soundex, Metaphone)
- [x] –ú–µ—Ç—Ä–∏–∫–∏ Precision, Recall, F1
- [x] –û—à–∏–±–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ —Ä–æ–¥–∞ (FPR ‚â§ 10%, FNR ‚â§ 5%)
- [x] –í—ã–±–æ—Ä –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏ –ø–æ –ø–æ–ª–Ω–æ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- [x] –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥—É–±–ª–µ–π
- [ ] –ü–æ–ª–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ —Å—Ç–µ–º–º–∏–Ω–≥)
- [ ] Seq2Seq –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
- [ ] BERT –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏

---

## üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
go test ./normalization/... -v

# –ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
go test ./normalization -run TestLevenshteinThreshold -v

# –ó–∞–ø—É—Å–∫ —Å –ø–æ–∫—Ä—ã—Ç–∏–µ–º
go test ./normalization/... -cover
```

---

## üìù –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã**. –¢–µ—Å—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –¥–æ–∫—É–º–µ–Ω—Ç–∞.

