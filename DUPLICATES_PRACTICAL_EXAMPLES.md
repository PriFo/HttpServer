# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–µ–π

## –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 2025-01-20

---

## üìö –ü—Ä–∏–º–µ—Ä—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 655-663)**:
```
"–¥–æ–º" ‚Üí "—Ç–æ–º" (1 –∑–∞–º–µ–Ω–∞)
–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ = 1
–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: –µ—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ ‚â§ 2, —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–æ—Ö–æ–∂–∏–º–∏
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ –∫–æ–¥–µ**:

```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    // –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer := normalization.NewDuplicateAnalyzer()
    
    // –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: "–¥–æ–º" vs "—Ç–æ–º"
    items := []normalization.DuplicateItem{
        {ID: 1, NormalizedName: "–¥–æ–º", Code: "001"},
        {ID: 2, NormalizedName: "—Ç–æ–º", Code: "002"},
    }
    
    // –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    groups := analyzer.AnalyzeDuplicates(items)
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if len(groups) > 0 {
        fmt.Printf("–ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã! –°—Ö–æ–∂–µ—Å—Ç—å: %.2f\n", 
            groups[0].SimilarityScore)
    }
    
    // –ü—Ä—è–º–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
    import "httpserver/normalization/algorithms"
    similarity := algorithms.LevenshteinSimilarity("–¥–æ–º", "—Ç–æ–º")
    fmt.Printf("–°—Ö–æ–∂–µ—Å—Ç—å –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞: %.2f\n", similarity)
    // –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: ~0.67 (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ 1 –∏–∑ 3 —Å–∏–º–≤–æ–ª–æ–≤)
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/duplicate_analyzer.go:828`

---

### –ü—Ä–∏–º–µ—Ä 2: N-–≥—Ä–∞–º–º—ã (Bigrams)

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 684-700)**:
```
–î–ª—è "–∑–≤–µ–Ω–æ": [–∑–≤, –≤–µ, –µ–Ω, –Ω–æ] (–±–∏–≥—Ä–∞–º–º—ã, N=2)
–î–ª—è "–∑–≤–µ–Ω–æ" –∏ "–∑–µ—Ä–Ω–æ":
- "–∑–≤–µ–Ω–æ": [–∑–≤, –≤–µ, –µ–Ω, –Ω–æ]
- "–∑–µ—Ä–Ω–æ": [–∑–µ, –µ—Ä, —Ä–Ω, –Ω–æ]
- –û–±—â–µ–µ: [–Ω–æ] ‚Äî 1 –≥—Ä–∞–º–º–∞
- –°—ë—Ä–µ–Ω—Å–µ–Ω = 2 √ó 1 / (4 + 4) = 0.25
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ –∫–æ–¥–µ**:

```go
package main

import (
    "fmt"
    "httpserver/normalization/algorithms"
)

func main() {
    // –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –±–∏–≥—Ä–∞–º–º
    gen := algorithms.NewNGramGenerator(2)
    
    // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º N-–≥—Ä–∞–º–º—ã –¥–ª—è "–∑–≤–µ–Ω–æ"
    ngrams1 := gen.Generate("–∑–≤–µ–Ω–æ")
    fmt.Printf("N-–≥—Ä–∞–º–º—ã –¥–ª—è '–∑–≤–µ–Ω–æ': %v\n", ngrams1)
    // –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: ["–∑–≤", "–≤–µ", "–µ–Ω", "–Ω–æ"] –∏–ª–∏ –ø–æ—Ö–æ–∂–µ–µ
    
    // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º N-–≥—Ä–∞–º–º—ã –¥–ª—è "–∑–µ—Ä–Ω–æ"
    ngrams2 := gen.Generate("–∑–µ—Ä–Ω–æ")
    fmt.Printf("N-–≥—Ä–∞–º–º—ã –¥–ª—è '–∑–µ—Ä–Ω–æ': %v\n", ngrams2)
    
    // –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –°—ë—Ä–µ–Ω—Å–µ–Ω—É (—á–µ—Ä–µ–∑ Jaccard)
    similarity := gen.Similarity("–∑–≤–µ–Ω–æ", "–∑–µ—Ä–Ω–æ")
    fmt.Printf("–°—Ö–æ–∂–µ—Å—Ç—å N-–≥—Ä–∞–º–º: %.2f\n", similarity)
    // –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: ~0.25 (–∫–∞–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)
    
    // –ò–ª–∏ —á–µ—Ä–µ–∑ FuzzyAlgorithms
    import "httpserver/normalization"
    fa := normalization.NewFuzzyAlgorithms()
    bigramSim := fa.BigramSimilarity("–∑–≤–µ–Ω–æ", "–∑–µ—Ä–Ω–æ")
    fmt.Printf("Bigram similarity: %.2f\n", bigramSim)
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/fuzzy_algorithms.go:38` –∏ `normalization/algorithms/ngram.go`

---

### –ü—Ä–∏–º–µ—Ä 3: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 600-623)**:

#### 1.1 –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –µ–¥–∏–Ω–æ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
```
"–ú–ê–°–õ–û –°–õ–ò–í–û–ß–ù–û–ï" ‚Üí "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ"
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "strings"
    "httpserver/normalization"
)

func main() {
    // –°–æ–∑–¥–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä
    normalizer := normalization.NewNameNormalizer()
    
    // –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    input := "–ú–ê–°–õ–û –°–õ–ò–í–û–ß–ù–û–ï"
    normalized := normalizer.NormalizeName(input)
    fmt.Printf("–ò—Å—Ö–æ–¥–Ω–æ–µ: %s\n", input)
    fmt.Printf("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ: %s\n", normalized)
    // –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ"
}
```

#### 1.2 –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
```
"  –º–∞—Å–ª–æ   —Å–ª–∏–≤–æ—á–Ω–æ–µ  " ‚Üí "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ"
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
func main() {
    normalizer := normalization.NewNameNormalizer()
    
    input := "  –º–∞—Å–ª–æ   —Å–ª–∏–≤–æ—á–Ω–æ–µ  "
    normalized := normalizer.NormalizeName(input)
    fmt.Printf("–ò—Å—Ö–æ–¥–Ω–æ–µ: '%s'\n", input)
    fmt.Printf("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ: '%s'\n", normalized)
    // –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ"
}
```

#### 1.3 –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
```
"–ú–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ, 82%" ‚Üí "–ú–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ 82"
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
func main() {
    normalizer := normalization.NewNameNormalizer()
    
    input := "–ú–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ, 82%"
    normalized := normalizer.NormalizeName(input)
    fmt.Printf("–ò—Å—Ö–æ–¥–Ω–æ–µ: %s\n", input)
    fmt.Printf("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ: %s\n", normalized)
    // –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ" (—á–∏—Å–ª–∞ —Ç–æ–∂–µ —É–¥–∞–ª—è—é—Ç—Å—è)
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/name_normalizer.go:43`

---

### –ü—Ä–∏–º–µ—Ä 4: –°—Ç–µ–º–º–∏–Ω–≥

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 630-632)**:
```
"–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è" + "–Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω" ‚Üí –æ–±–∞ –∏–º–µ—é—Ç –∫–æ—Ä–µ–Ω—å "–Ω–æ—Ä–º–∞–ª–∏–∑"
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "httpserver/normalization/algorithms"
)

func main() {
    // –°–æ–∑–¥–∞–µ–º —Å—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    stemmer := algorithms.NewRussianStemmer()
    
    // –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    word1 := "–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è"
    word2 := "–Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
    
    stem1 := stemmer.Stem(word1)
    stem2 := stemmer.Stem(word2)
    
    fmt.Printf("–°–ª–æ–≤–æ 1: %s ‚Üí –∫–æ—Ä–µ–Ω—å: %s\n", word1, stem1)
    fmt.Printf("–°–ª–æ–≤–æ 2: %s ‚Üí –∫–æ—Ä–µ–Ω—å: %s\n", word2, stem2)
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ—Ä–Ω–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç (–∏–ª–∏ –ø–æ—Ö–æ–∂–∏)
    if stem1 == stem2 {
        fmt.Println("–ö–æ—Ä–Ω–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç!")
    } else {
        fmt.Printf("–ö–æ—Ä–Ω–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è, –Ω–æ –ø–æ—Ö–æ–∂–∏: %s vs %s\n", stem1, stem2)
    }
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/algorithms/stemmer.go:47`

---

### –ü—Ä–∏–º–µ—Ä 5: –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 636-638)**:
```
"–º–∞—Å–ª–∞–º–∏" ‚Üí "–º–∞—Å–ª–æ", "—Å–ª–∏–≤–æ—á–Ω–æ–≥–æ" ‚Üí "—Å–ª–∏–≤–æ—á–Ω—ã–π"
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "httpserver/normalization/algorithms"
)

func main() {
    // –í–ê–ñ–ù–û: –ü–æ–ª–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞
    // –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–µ–º–º–∏–Ω–≥ –∫–∞–∫ –∑–∞–º–µ–Ω–∞
    
    stemmer := algorithms.NewRussianStemmer()
    
    // –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    word1 := "–º–∞—Å–ª–∞–º–∏"
    word2 := "—Å–ª–∏–≤–æ—á–Ω–æ–≥–æ"
    
    // –°—Ç–µ–º–º–∏–Ω–≥ (–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏)
    stem1 := stemmer.Stem(word1)
    stem2 := stemmer.Stem(word2)
    
    fmt.Printf("–°–ª–æ–≤–æ 1: %s ‚Üí —Å—Ç–µ–º: %s\n", word1, stem1)
    fmt.Printf("–°–ª–æ–≤–æ 2: %s ‚Üí —Å—Ç–µ–º: %s\n", word2, stem2)
    
    // TODO: –ü–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏:
    // lemmatizer := algorithms.NewRussianLemmatizer()
    // lemma1 := lemmatizer.Lemmatize("–º–∞—Å–ª–∞–º–∏") // ‚Üí "–º–∞—Å–ª–æ"
    // lemma2 := lemmatizer.Lemmatize("—Å–ª–∏–≤–æ—á–Ω–æ–≥–æ") // ‚Üí "—Å–ª–∏–≤–æ—á–Ω—ã–π"
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (—Ç–æ–ª—å–∫–æ —Å—Ç–µ–º–º–∏–Ω–≥), –ø–æ–ª–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤ –ø–ª–∞–Ω–∞—Ö

---

### –ü—Ä–∏–º–µ—Ä 6: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 641-644)**:
```
"–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ –¥–ª—è –≥–æ—Ç–æ–≤–∫–∏" ‚Üí "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ –≥–æ—Ç–æ–≤–∫–∞"
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    analyzer := normalization.NewDuplicateAnalyzer()
    
    // –ù–∞—Å—Ç—Ä–æ–π–∫–∞: –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    analyzer.wordBasedUseStopWords = false
    
    text := "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ –¥–ª—è –≥–æ—Ç–æ–≤–∫–∏"
    
    // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏
    // –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤
    items := []normalization.DuplicateItem{
        {ID: 1, NormalizedName: text, Code: "001"},
        {ID: 2, NormalizedName: "–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ –≥–æ—Ç–æ–≤–∫–∞", Code: "002"},
    }
    
    // –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ "–¥–ª—è" –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
    groups := analyzer.AnalyzeWordBasedDuplicates(items)
    
    if len(groups) > 0 {
        fmt.Println("–ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –æ–±—â–∏–º —Å–ª–æ–≤–∞–º (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è)")
    }
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/duplicate_analyzer.go:752`

---

### –ü—Ä–∏–º–µ—Ä 7: –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 702-706)**:
```
Soundex: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞ –≤ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥
Metaphone: –±–æ–ª–µ–µ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–∞—è —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: –ø–æ–∏—Å–∫ –¥—É–±–ª–µ–π, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "httpserver/normalization/algorithms"
)

func main() {
    // Soundex –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    soundex := algorithms.NewSoundexRU()
    
    word1 := "–ò–≤–∞–Ω–æ–≤"
    word2 := "Ivanov" // –ª–∞—Ç–∏–Ω–∏—Ü–∞
    
    code1 := soundex.Encode(word1)
    code2 := soundex.Encode(word2)
    
    fmt.Printf("Soundex –∫–æ–¥ –¥–ª—è '%s': %s\n", word1, code1)
    fmt.Printf("Soundex –∫–æ–¥ –¥–ª—è '%s': %s\n", word2, code2)
    
    // –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
    similarity := soundex.Similarity(word1, word2)
    fmt.Printf("–§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: %.2f\n", similarity)
    
    // Metaphone (–±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π)
    metaphone := algorithms.NewMetaphoneRU()
    meta1 := metaphone.Encode(word1)
    meta2 := metaphone.Encode(word2)
    
    fmt.Printf("Metaphone –∫–æ–¥ –¥–ª—è '%s': %s\n", word1, meta1)
    fmt.Printf("Metaphone –∫–æ–¥ –¥–ª—è '%s': %s\n", word2, meta2)
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/algorithms/soundex_ru.go` –∏ `metaphone_ru.go`

---

### –ü—Ä–∏–º–µ—Ä 8: –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 805-813)**:
```
1. –û—á–∏—Å—Ç–∫–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä ‚Üí —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ ‚Üí —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
2. –õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Üí –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è ‚Üí —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ –¥—É–±–ª–µ–π: —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ + N-–≥—Ä–∞–º–º—ã ‚Üí –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ: —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è ‚Üí NER
5. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: Seq2Seq ‚Üí BiLSTM ‚Üí BERT
6. –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è: –≤—ã–±–æ—Ä –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏ ‚Üí –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    // –°–æ–∑–¥–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä
    nsi := normalization.NewNSINormalizer()
    
    // –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    items := []normalization.DuplicateItem{
        {
            ID:             1,
            Code:           "WBC00Z0002",
            NormalizedName: "WBC00Z0002 –ö–∞–±–µ–ª—å –í–í–ì 3x2.5 120mm",
            Category:       "—Å—Ç—Ä–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã",
            QualityScore:   0.9,
        },
        {
            ID:             2,
            Code:           "WBC00Z0003",
            NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5",
            Category:       "—Å—Ç—Ä–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã",
            QualityScore:   0.85,
        },
        {
            ID:             3,
            Code:           "001",
            NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π 500–≥—Ä",
            Category:       "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
            QualityScore:   0.88,
        },
        {
            ID:             4,
            Code:           "002",
            NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π", // –æ–ø–µ—á–∞—Ç–∫–∞
            Category:       "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
            QualityScore:   0.80,
        },
    }
    
    // –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–µ–π
    config := normalization.DefaultDuplicateDetectionConfig()
    config.UseExactMatching = true
    config.UseFuzzyMatching = true
    config.Threshold = 0.85
    config.MergeOverlapping = true
    config.MinConfidence = 0.8
    
    // –®–∞–≥ 1-2: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    for i := range items {
        normalized := nsi.NormalizeName(items[i].NormalizedName, 
            normalization.NormalizationOptions{})
        items[i].NormalizedName = normalized
    }
    
    // –®–∞–≥ 3: –ü–æ–∏—Å–∫ –¥—É–±–ª–µ–π
    groups := nsi.FindDuplicates(items, config)
    
    fmt.Printf("–ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: %d\n\n", len(groups))
    
    // –®–∞–≥ 4-6: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏
    for i, group := range groups {
        fmt.Printf("–ì—Ä—É–ø–ø–∞ %d:\n", i+1)
        fmt.Printf("  –¢–∏–ø: %s\n", group.Type)
        fmt.Printf("  –°—Ö–æ–∂–µ—Å—Ç—å: %.2f\n", group.SimilarityScore)
        fmt.Printf("  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: %.2f\n", group.Confidence)
        fmt.Printf("  –ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å ID: %d\n", group.SuggestedMaster)
        fmt.Printf("  –≠–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ: %d\n", len(group.Items))
        fmt.Printf("  –ü—Ä–∏—á–∏–Ω–∞: %s\n", group.Reason)
        fmt.Println()
    }
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/nsi_normalizer.go`

---

### –ü—Ä–∏–º–µ—Ä 9: –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ (Precision, Recall, F1)

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 45-47, 87-89)**:
```
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1-–º–µ—Ä–∞ = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    // –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏
    metrics := normalization.NewEvaluationMetrics()
    
    // –†–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥—É–±–ª–∏)
    actual := []normalization.DuplicateGroup{
        {
            GroupID: "actual_1",
            Items: []normalization.DuplicateItem{
                {ID: 1, NormalizedName: "–º–æ–ª–æ—Ç–æ–∫"},
                {ID: 2, NormalizedName: "–º–æ–ª–æ—Ç–∞–∫"},
            },
        },
    }
    
    // –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥—É–±–ª–∏ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞)
    predicted := []normalization.DuplicateGroup{
        {
            GroupID: "predicted_1",
            Items: []normalization.DuplicateItem{
                {ID: 1, NormalizedName: "–º–æ–ª–æ—Ç–æ–∫"},
                {ID: 2, NormalizedName: "–º–æ–ª–æ—Ç–∞–∫"},
            },
        },
    }
    
    // –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    result := metrics.EvaluateAlgorithm(predicted, actual)
    
    fmt.Printf("Precision (–¢–æ—á–Ω–æ—Å—Ç—å): %.4f\n", result.Precision)
    fmt.Printf("Recall (–ü–æ–ª–Ω–æ—Ç–∞): %.4f\n", result.Recall)
    fmt.Printf("F1-–º–µ—Ä–∞: %.4f\n", result.F1Score)
    fmt.Printf("Accuracy: %.4f\n", result.Accuracy)
    fmt.Printf("\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:\n")
    fmt.Printf("  TP (True Positive): %d\n", result.ConfusionMatrix.TruePositive)
    fmt.Printf("  FP (False Positive): %d\n", result.ConfusionMatrix.FalsePositive)
    fmt.Printf("  FN (False Negative): %d\n", result.ConfusionMatrix.FalseNegative)
    fmt.Printf("  TN (True Negative): %d\n", result.ConfusionMatrix.TrueNegative)
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/evaluation_metrics.go:24`

---

### –ü—Ä–∏–º–µ—Ä 10: –í—ã–±–æ—Ä –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏

**–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—Ç—Ä–æ–∫–∞ 796-798)**:
```
–ê–ª–≥–æ—Ä–∏—Ç–º: –∏–∑ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –≤—ã–±–∏—Ä–∞—Ç—å –∑–∞–ø–∏—Å—å —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –ø–æ–ª–Ω–æ—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ –∞—Ç—Ä–∏–±—É—Ç–∞–º
```

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    analyzer := normalization.NewDuplicateAnalyzer()
    
    // –ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    items := []normalization.DuplicateItem{
        {
            ID:             1,
            NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5",
            QualityScore:   0.9,
            MergedCount:    0,
            ProcessingLevel: "ai_enhanced",
        },
        {
            ID:             2,
            NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥",
            QualityScore:   0.7,
            MergedCount:    0,
            ProcessingLevel: "basic",
        },
        {
            ID:             3,
            NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5 120mm –º–µ–¥–Ω—ã–π",
            QualityScore:   0.95,
            MergedCount:    2, // —É–∂–µ –æ–±—ä–µ–¥–∏–Ω—è–ª –¥—Ä—É–≥–∏–µ –∑–∞–ø–∏—Å–∏
            ProcessingLevel: "benchmark",
        },
    }
    
    // –í—ã–±–∏—Ä–∞–µ–º –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å
    masterID := analyzer.selectMasterRecord(items)
    
    fmt.Printf("–í—ã–±—Ä–∞–Ω–Ω–∞—è –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å ID: %d\n", masterID)
    
    // –ù–∞—Ö–æ–¥–∏–º –∑–∞–ø–∏—Å—å
    var master normalization.DuplicateItem
    for _, item := range items {
        if item.ID == masterID {
            master = item
            break
        }
    }
    
    fmt.Printf("–ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å:\n")
    fmt.Printf("  –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: %s\n", master.NormalizedName)
    fmt.Printf("  –ö–∞—á–µ—Å—Ç–≤–æ: %.2f\n", master.QualityScore)
    fmt.Printf("  –£—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: %s\n", master.ProcessingLevel)
    fmt.Printf("  –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: %d\n", master.MergedCount)
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ `normalization/duplicate_analyzer.go:626`

---

## üß™ –¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü–æ–∏—Å–∫ –¥—É–±–ª–µ–π —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏

```go
func TestDuplicatesWithTypos(t *testing.T) {
    analyzer := normalization.NewDuplicateAnalyzer()
    
    items := []normalization.DuplicateItem{
        {ID: 1, NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
        {ID: 2, NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"}, // –æ–ø–µ—á–∞—Ç–∫–∞
        {ID: 3, NormalizedName: "–∫–∞–±–µ–ª—å –º–µ–¥–Ω—ã–π"},
    }
    
    groups := analyzer.AnalyzeDuplicates(items)
    
    // –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω–∞ –≥—Ä—É–ø–ø–∞ —Å ID 1 –∏ 2
    found := false
    for _, group := range groups {
        if len(group.Items) == 2 && 
           (group.Items[0].ID == 1 || group.Items[1].ID == 1) &&
           (group.Items[0].ID == 2 || group.Items[1].ID == 2) {
            found = true
            break
        }
    }
    
    if !found {
        t.Error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã —Å –æ–ø–µ—á–∞—Ç–∫–æ–π")
    }
}
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞

```go
func TestQualityMetrics(t *testing.T) {
    metrics := normalization.NewEvaluationMetrics()
    
    matrix := normalization.ConfusionMatrix{
        TruePositive:  90,
        FalsePositive: 10,
        FalseNegative: 5,
        TrueNegative:  895,
    }
    
    result := metrics.CalculateMetrics(matrix)
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
    // –û—à–∏–±–∫–∏ –ø–µ—Ä–≤–æ–≥–æ —Ä–æ–¥–∞ –Ω–µ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–≤—ã—à–∞—Ç—å 10%
    if result.FalsePositiveRate > 0.10 {
        t.Errorf("FPR –ø—Ä–µ–≤—ã—à–∞–µ—Ç 10%%: %.2f%%", result.FalsePositiveRate*100)
    }
    
    // –û—à–∏–±–∫–∏ –≤—Ç–æ—Ä–æ–≥–æ —Ä–æ–¥–∞ –Ω–µ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–≤—ã—à–∞—Ç—å 5%
    if result.FalseNegativeRate > 0.05 {
        t.Errorf("FNR –ø—Ä–µ–≤—ã—à–∞–µ—Ç 5%%: %.2f%%", result.FalseNegativeRate*100)
    }
}
```

---

## üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞: –î–æ–∫—É–º–µ–Ω—Ç vs –†–µ–∞–ª–∏–∑–∞—Ü–∏—è

| –ê–ª–≥–æ—Ä–∏—Ç–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ | –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ | –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ | –§–∞–π–ª —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ |
|----------------------|---------------------|-------------|-----------------|
| –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ | "–¥–æ–º" ‚Üí "—Ç–æ–º" (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ 1) | ‚úÖ | `duplicate_analyzer.go:828` |
| N-–≥—Ä–∞–º–º—ã | "–∑–≤–µ–Ω–æ" vs "–∑–µ—Ä–Ω–æ" (0.25) | ‚úÖ | `fuzzy_algorithms.go:38` |
| –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É | "–ú–ê–°–õ–û" ‚Üí "–º–∞—Å–ª–æ" | ‚úÖ | `name_normalizer.go:50` |
| –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ | "  –º–∞—Å–ª–æ  " ‚Üí "–º–∞—Å–ª–æ" | ‚úÖ | `name_normalizer.go:71` |
| –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ | "–º–∞—Å–ª–æ, 82%" ‚Üí "–º–∞—Å–ª–æ" | ‚úÖ | `name_normalizer.go` |
| –°—Ç–µ–º–º–∏–Ω–≥ | "–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è" ‚Üí "–Ω–æ—Ä–º–∞–ª–∏–∑" | ‚úÖ | `algorithms/stemmer.go` |
| –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è | "–º–∞—Å–ª–∞–º–∏" ‚Üí "–º–∞—Å–ª–æ" | ‚ö†Ô∏è | –¢–æ–ª—å–∫–æ —Å—Ç–µ–º–º–∏–Ω–≥ |
| –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ | "–¥–ª—è –≥–æ—Ç–æ–≤–∫–∏" ‚Üí "–≥–æ—Ç–æ–≤–∫–∞" | ‚úÖ | `duplicate_analyzer.go:752` |
| Soundex | –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–¥—ã | ‚úÖ | `algorithms/soundex_ru.go` |
| Metaphone | –£–ª—É—á—à–µ–Ω–Ω—ã–π Soundex | ‚úÖ | `algorithms/metaphone_ru.go` |
| Precision/Recall/F1 | –§–æ—Ä–º—É–ª—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ | ‚úÖ | `evaluation_metrics.go:24` |
| –í—ã–±–æ—Ä –º–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å–∏ | –ü–æ –ø–æ–ª–Ω–æ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ | ‚úÖ | `duplicate_analyzer.go:626` |

---

## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ **—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ —Ä–∞–±–æ—Ç–∞—é—Ç**. –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ - –ø–æ–ª–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–µ–º–º–∏–Ω–≥ –∫–∞–∫ –∑–∞–º–µ–Ω–∞).

–í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π.

