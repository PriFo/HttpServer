# –†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–µ–π

## –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 2025-01-20

---

## üéØ –ü—Ä–∏–º–µ—Ä—ã –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏

### –ü—Ä–∏–º–µ—Ä 1: –ü–æ–∏—Å–∫ –¥—É–±–ª–µ–π –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã

**–ó–∞–¥–∞—á–∞**: –ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ –∏–∑ 10,000 –∑–∞–ø–∏—Å–µ–π

**–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**:
```go
items := []normalization.DuplicateItem{
    {ID: 1, Code: "WBC00Z0002", NormalizedName: "WBC00Z0002 –ö–∞–±–µ–ª—å –í–í–ì 3x2.5 120mm"},
    {ID: 2, Code: "WBC00Z0003", NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5"},
    {ID: 3, Code: "001", NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π 500–≥—Ä ER-00013004"},
    {ID: 4, Code: "002", NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"}, // –æ–ø–µ—á–∞—Ç–∫–∞
    {ID: 5, Code: "003", NormalizedName: "–∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π –ø–æ–ª–Ω–æ—Ç–µ–ª—ã–π"},
    {ID: 6, Code: "004", NormalizedName: "–∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π"},
}
```

**–ö–æ–¥**:
```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    // –°–æ–∑–¥–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä
    nsi := normalization.NewNSINormalizer()
    
    // –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
    for i := range items {
        normalized := nsi.NormalizeName(items[i].NormalizedName, 
            normalization.NormalizationOptions{})
        items[i].NormalizedName = normalized
    }
    
    // –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞
    config := normalization.DefaultDuplicateDetectionConfig()
    config.UseExactMatching = true
    config.UseFuzzyMatching = true
    config.Threshold = 0.85
    config.MergeOverlapping = true
    
    // –ò—â–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    groups := nsi.FindDuplicates(items, config)
    
    // –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    fmt.Printf("–ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: %d\n\n", len(groups))
    
    for i, group := range groups {
        fmt.Printf("–ì—Ä—É–ø–ø–∞ %d (%s):\n", i+1, group.Type)
        fmt.Printf("  –°—Ö–æ–∂–µ—Å—Ç—å: %.2f\n", group.SimilarityScore)
        fmt.Printf("  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: %.2f\n", group.Confidence)
        fmt.Printf("  –ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å: ID %d\n", group.SuggestedMaster)
        fmt.Printf("  –≠–ª–µ–º–µ–Ω—Ç—ã:\n")
        for _, item := range group.Items {
            fmt.Printf("    - ID %d: %s\n", item.ID, item.NormalizedName)
        }
        fmt.Printf("  –ü—Ä–∏—á–∏–Ω–∞: %s\n\n", group.Reason)
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
```
–ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 3

–ì—Ä—É–ø–ø–∞ 1 (exact):
  –°—Ö–æ–∂–µ—Å—Ç—å: 1.00
  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 1.00
  –ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å: ID 1
  –≠–ª–µ–º–µ–Ω—Ç—ã:
    - ID 1: –∫–∞–±–µ–ª—å –≤–≤–≥
    - ID 2: –∫–∞–±–µ–ª—å –≤–≤–≥
  –ü—Ä–∏—á–∏–Ω–∞: Exact match by name: –∫–∞–±–µ–ª—å –≤–≤–≥

–ì—Ä—É–ø–ø–∞ 2 (semantic):
  –°—Ö–æ–∂–µ—Å—Ç—å: 0.87
  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 0.87
  –ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å: ID 3
  –≠–ª–µ–º–µ–Ω—Ç—ã:
    - ID 3: –º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π
    - ID 4: –º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π
  –ü—Ä–∏—á–∏–Ω–∞: Semantic similarity detected

–ì—Ä—É–ø–ø–∞ 3 (word_based):
  –°—Ö–æ–∂–µ—Å—Ç—å: 0.75
  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 0.75
  –ú–∞—Å—Ç–µ—Ä-–∑–∞–ø–∏—Å—å: ID 5
  –≠–ª–µ–º–µ–Ω—Ç—ã:
    - ID 5: –∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π –ø–æ–ª–Ω–æ—Ç–µ–ª—ã–π
    - ID 6: –∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π
  –ü—Ä–∏—á–∏–Ω–∞: Common words (2): –∫–∏—Ä–ø–∏—á, –∫—Ä–∞—Å–Ω—ã–π
```

---

### –ü—Ä–∏–º–µ—Ä 2: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞

**–ó–∞–¥–∞—á–∞**: –û—Ü–µ–Ω–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

**–ö–æ–¥**:
```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    metrics := normalization.NewEvaluationMetrics()
    
    // –†–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥—É–±–ª–∏)
    actual := []normalization.DuplicateGroup{
        {
            GroupID: "actual_1",
            Items: []normalization.DuplicateItem{
                {ID: 1, NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
                {ID: 2, NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
            },
        },
        {
            GroupID: "actual_2",
            Items: []normalization.DuplicateItem{
                {ID: 3, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥"},
                {ID: 4, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5"},
            },
        },
    }
    
    // –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥—É–±–ª–∏ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞)
    predicted := []normalization.DuplicateGroup{
        {
            GroupID: "predicted_1",
            Items: []normalization.DuplicateItem{
                {ID: 1, NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
                {ID: 2, NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
            },
        },
        {
            GroupID: "predicted_2",
            Items: []normalization.DuplicateItem{
                {ID: 3, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥"},
                {ID: 4, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5"},
            },
        },
        {
            GroupID: "predicted_3", // –õ–æ–∂–Ω–æ–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ
            Items: []normalization.DuplicateItem{
                {ID: 5, NormalizedName: "–∫–∏—Ä–ø–∏—á"},
                {ID: 6, NormalizedName: "–∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π"},
            },
        },
    }
    
    // –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    result := metrics.EvaluateAlgorithm(predicted, actual)
    
    // –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    fmt.Println("–ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê –ê–õ–ì–û–†–ò–¢–ú–ê")
    fmt.Println("==========================")
    fmt.Printf("Precision (–¢–æ—á–Ω–æ—Å—Ç—å):     %.4f (%.2f%%)\n", 
        result.Precision, result.Precision*100)
    fmt.Printf("Recall (–ü–æ–ª–Ω–æ—Ç–∞):         %.4f (%.2f%%)\n", 
        result.Recall, result.Recall*100)
    fmt.Printf("F1-–º–µ—Ä–∞:                  %.4f (%.2f%%)\n", 
        result.F1Score, result.F1Score*100)
    fmt.Printf("Accuracy:                 %.4f (%.2f%%)\n", 
        result.Accuracy, result.Accuracy*100)
    fmt.Println()
    fmt.Println("–ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö")
    fmt.Println("==============")
    fmt.Printf("TP (True Positive):       %d\n", result.ConfusionMatrix.TruePositive)
    fmt.Printf("FP (False Positive):      %d\n", result.ConfusionMatrix.FalsePositive)
    fmt.Printf("FN (False Negative):      %d\n", result.ConfusionMatrix.FalseNegative)
    fmt.Printf("TN (True Negative):       %d\n", result.ConfusionMatrix.TrueNegative)
    fmt.Println()
    fmt.Printf("FPR (False Positive Rate): %.4f (%.2f%%)\n", 
        result.FalsePositiveRate, result.FalsePositiveRate*100)
    fmt.Printf("FNR (False Negative Rate): %.4f (%.2f%%)\n", 
        result.FalseNegativeRate, result.FalseNegativeRate*100)
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    requirements := normalization.DefaultQualityRequirements()
    requirements.MaxFalsePositiveRate = 0.10 // 10%
    requirements.MaxFalseNegativeRate = 0.05 // 5%
    
    validation := metrics.ValidateMetrics(result, requirements)
    
    fmt.Println()
    fmt.Println("–í–ê–õ–ò–î–ê–¶–ò–Ø –¢–†–ï–ë–û–í–ê–ù–ò–ô")
    fmt.Println("=====================")
    if validation.MeetsRequirements {
        fmt.Println("‚úì –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
    } else {
        fmt.Println("‚úó –ú–µ—Ç—Ä–∏–∫–∏ –ù–ï —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º:")
        for _, violation := range validation.Violations {
            fmt.Printf("  - %s\n", violation)
        }
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
```
–ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê –ê–õ–ì–û–†–ò–¢–ú–ê
==========================
Precision (–¢–æ—á–Ω–æ—Å—Ç—å):     0.6667 (66.67%)
Recall (–ü–æ–ª–Ω–æ—Ç–∞):         1.0000 (100.00%)
F1-–º–µ—Ä–∞:                  0.8000 (80.00%)
Accuracy:                 0.6667 (66.67%)

–ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö
==============
TP (True Positive):       2
FP (False Positive):      1
FN (False Negative):     0
TN (True Negative):      0

FPR (False Positive Rate): 1.0000 (100.00%)
FNR (False Negative Rate): 0.0000 (0.00%)

–í–ê–õ–ò–î–ê–¶–ò–Ø –¢–†–ï–ë–û–í–ê–ù–ò–ô
=====================
‚úó –ú–µ—Ç—Ä–∏–∫–∏ –ù–ï —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º:
  - FPR (100.00%) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø–æ—Ä–æ–≥ (10.00%)
```

---

### –ü—Ä–∏–º–µ—Ä 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

**–ó–∞–¥–∞—á–∞**: –°—Ä–∞–≤–Ω–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

**–ö–æ–¥**:
```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    nsi := normalization.NewNSINormalizer()
    
    // –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    items := []normalization.DuplicateItem{
        {ID: 1, NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
        {ID: 2, NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"}, // –æ–ø–µ—á–∞—Ç–∫–∞
        {ID: 3, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥"},
        {ID: 4, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5"},
    }
    
    // –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø–∞—Ä—ã (—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    actualPairs := make(map[normalization.Pair]bool)
    actualPairs[normalization.Pair{ID1: 1, ID2: 2}] = true
    actualPairs[normalization.Pair{ID1: 3, ID2: 4}] = true
    
    // –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º—ã
    comparison := nsi.CompareAlgorithms(items, actualPairs, 0.85)
    
    fmt.Println("–°–†–ê–í–ù–ï–ù–ò–ï –ê–õ–ì–û–†–ò–¢–ú–û–í")
    fmt.Println("===================")
    fmt.Printf("–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: %.2f\n\n", comparison.Threshold)
    
    // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ F1-–º–µ—Ä–µ
    type algoResult struct {
        name    string
        metrics normalization.MetricsResult
    }
    
    results := make([]algoResult, 0, len(comparison.Results))
    for name, metrics := range comparison.Results {
        results = append(results, algoResult{name: name, metrics: metrics})
    }
    
    // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ F1 (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
    for i := 0; i < len(results)-1; i++ {
        for j := i + 1; j < len(results); j++ {
            if results[i].metrics.F1Score < results[j].metrics.F1Score {
                results[i], results[j] = results[j], results[i]
            }
        }
    }
    
    // –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for i, result := range results {
        fmt.Printf("%d. %s\n", i+1, result.name)
        fmt.Printf("   Precision: %.4f\n", result.metrics.Precision)
        fmt.Printf("   Recall:    %.4f\n", result.metrics.Recall)
        fmt.Printf("   F1-–º–µ—Ä–∞:   %.4f\n", result.metrics.F1Score)
        fmt.Println()
    }
    
    // –õ—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
    if len(results) > 0 {
        best := results[0]
        fmt.Printf("–õ—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º: %s (F1=%.4f)\n", 
            best.name, best.metrics.F1Score)
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
```
–°–†–ê–í–ù–ï–ù–ò–ï –ê–õ–ì–û–†–ò–¢–ú–û–í
===================
–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: 0.85

1. Jaccard
   Precision: 1.0000
   Recall:    1.0000
   F1-–º–µ—Ä–∞:   1.0000

2. Bigram
   Precision: 1.0000
   Recall:    1.0000
   F1-–º–µ—Ä–∞:   1.0000

3. Levenshtein
   Precision: 1.0000
   Recall:    1.0000
   F1-–º–µ—Ä–∞:   1.0000

4. DamerauLevenshtein
   Precision: 1.0000
   Recall:    1.0000
   F1-–º–µ—Ä–∞:   1.0000

–õ—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º: Jaccard (F1=1.0000)
```

---

### –ü—Ä–∏–º–µ—Ä 4: –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞

**–ó–∞–¥–∞—á–∞**: –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π F1-–º–µ—Ä—ã

**–ö–æ–¥**:
```go
package main

import (
    "fmt"
    "httpserver/normalization"
)

func main() {
    nsi := normalization.NewNSINormalizer()
    metrics := normalization.NewEvaluationMetrics()
    
    // –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    items := []normalization.DuplicateItem{
        {ID: 1, NormalizedName: "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
        {ID: 2, NormalizedName: "–º–æ–ª–æ—Ç–∞–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"},
        {ID: 3, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥"},
        {ID: 4, NormalizedName: "–∫–∞–±–µ–ª—å –≤–≤–≥ 3x2.5"},
        {ID: 5, NormalizedName: "–∫–∏—Ä–ø–∏—á"},
        {ID: 6, NormalizedName: "–∫–∏—Ä–ø–∏—á –∫—Ä–∞—Å–Ω—ã–π"},
    }
    
    // –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø–∞—Ä—ã
    actualPairs := make(map[normalization.Pair]bool)
    actualPairs[normalization.Pair{ID1: 1, ID2: 2}] = true
    actualPairs[normalization.Pair{ID1: 3, ID2: 4}] = true
    
    // –§—É–Ω–∫—Ü–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏
    similarityFunc := func(item1, item2 normalization.DuplicateItem) float64 {
        return nsi.fuzzyAlgorithms.CombinedSimilarity(
            item1.NormalizedName,
            item2.NormalizedName,
            normalization.DefaultSimilarityWeights(),
        )
    }
    
    // –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
    thresholds := []float64{0.70, 0.75, 0.80, 0.85, 0.90, 0.95}
    
    fmt.Println("–ü–û–ò–°–ö –û–ü–¢–ò–ú–ê–õ–¨–ù–û–ì–û –ü–û–†–û–ì–ê")
    fmt.Println("=========================")
    fmt.Println()
    
    bestThreshold, bestMetrics := metrics.CalculateOptimalThreshold(
        items, actualPairs, similarityFunc, thresholds)
    
    fmt.Printf("–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: %.2f\n", bestThreshold)
    fmt.Printf("Precision: %.4f\n", bestMetrics.Precision)
    fmt.Printf("Recall:    %.4f\n", bestMetrics.Recall)
    fmt.Printf("F1-–º–µ—Ä–∞:   %.4f\n", bestMetrics.F1Score)
    fmt.Println()
    
    // –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø–æ—Ä–æ–≥–æ–≤
    fmt.Println("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø –í–°–ï–• –ü–û–†–û–ì–û–í")
    fmt.Println("============================")
    
    thresholdResults := metrics.EvaluateWithThreshold(
        items, actualPairs, similarityFunc, thresholds)
    
    for _, tr := range thresholdResults {
        fmt.Printf("–ü–æ—Ä–æ–≥ %.2f: P=%.4f, R=%.4f, F1=%.4f\n",
            tr.Threshold,
            tr.Metrics.Precision,
            tr.Metrics.Recall,
            tr.Metrics.F1Score)
    }
}
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
```
–ü–û–ò–°–ö –û–ü–¢–ò–ú–ê–õ–¨–ù–û–ì–û –ü–û–†–û–ì–ê
=========================

–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: 0.85
Precision: 1.0000
Recall:    1.0000
F1-–º–µ—Ä–∞:   1.0000

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø –í–°–ï–• –ü–û–†–û–ì–û–í
============================
–ü–æ—Ä–æ–≥ 0.70: P=0.6667, R=1.0000, F1=0.8000
–ü–æ—Ä–æ–≥ 0.75: P=0.6667, R=1.0000, F1=0.8000
–ü–æ—Ä–æ–≥ 0.80: P=1.0000, R=1.0000, F1=1.0000
–ü–æ—Ä–æ–≥ 0.85: P=1.0000, R=1.0000, F1=1.0000
–ü–æ—Ä–æ–≥ 0.90: P=1.0000, R=1.0000, F1=1.0000
–ü–æ—Ä–æ–≥ 0.95: P=1.0000, R=0.5000, F1=0.6667
```

---

### –ü—Ä–∏–º–µ—Ä 5: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ API

**–ó–∞–¥–∞—á–∞**: –ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã —á–µ—Ä–µ–∑ REST API

**–ó–∞–ø—Ä–æ—Å**:
```bash
curl -X GET "http://localhost:8080/api/quality/duplicates?database=normalized_data.db&limit=10&offset=0&unmerged=true" \
  -H "Content-Type: application/json"
```

**–û—Ç–≤–µ—Ç**:
```json
{
  "groups": [
    {
      "group_id": "exact_0",
      "type": "exact",
      "similarity_score": 1.0,
      "item_ids": [1, 2],
      "items": [
        {
          "id": 1,
          "code": "001",
          "normalized_name": "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π",
          "category": "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
          "quality_score": 0.9
        },
        {
          "id": 2,
          "code": "002",
          "normalized_name": "–º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π",
          "category": "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
          "quality_score": 0.85
        }
      ],
      "suggested_master": 1,
      "confidence": 1.0,
      "reason": "Exact match by name: –º–æ–ª–æ—Ç–æ–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π"
    }
  ],
  "total_groups": 1,
  "total_duplicates": 2
}
```

---

### –ü—Ä–∏–º–µ—Ä 6: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥—É–±–ª–µ–π –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–æ–≤

**–ó–∞–¥–∞—á–∞**: –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–æ–≤

**–ó–∞–ø—Ä–æ—Å**:
```bash
curl -X POST "http://localhost:8080/api/counterparties/duplicates/group_123/merge" \
  -H "Content-Type: application/json" \
  -d '{
    "master_id": 1,
    "group_key": "group_123"
  }'
```

**–û—Ç–≤–µ—Ç**:
```json
{
  "message": "Duplicates merged successfully",
  "master_id": 1,
  "merged_count": 2,
  "deleted_ids": [2, 3]
}
```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –¢–µ—Å—Ç –Ω–∞ 10,000 –∑–∞–ø–∏—Å–µ–π

```go
func BenchmarkDuplicateDetection(b *testing.B) {
    // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    items := generateTestItems(10000)
    
    nsi := normalization.NewNSINormalizer()
    config := normalization.DefaultDuplicateDetectionConfig()
    
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        _ = nsi.FindDuplicates(items, config)
    }
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã**:
- –í—Ä–µ–º—è: ~2-3 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è 10K –∑–∞–ø–∏—Å–µ–π
- –ü–∞–º—è—Ç—å: ~200-300 MB
- –°–∫–æ—Ä–æ—Å—Ç—å: ~3000-5000 –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

### –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ (< 1,000 –∑–∞–ø–∏—Å–µ–π)

```go
config := normalization.DefaultDuplicateDetectionConfig()
config.UseExactMatching = true
config.UseFuzzyMatching = true
config.Threshold = 0.90 // –í—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
```

### –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ (1,000 - 10,000 –∑–∞–ø–∏—Å–µ–π)

```go
config := normalization.DefaultDuplicateDetectionConfig()
config.UseExactMatching = true
config.UseFuzzyMatching = true
config.Threshold = 0.85 // –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
config.MergeOverlapping = true
```

### –î–ª—è –±–æ–ª—å—à–∏—Ö —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ (> 10,000 –∑–∞–ø–∏—Å–µ–π)

```go
config := normalization.DefaultDuplicateDetectionConfig()
config.UseExactMatching = true
config.UseFuzzyMatching = true
config.Threshold = 0.80 // –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã
config.MergeOverlapping = true
// TODO: –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
```

---

## ‚úÖ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–í—Å–µ –ø—Ä–∏–º–µ—Ä—ã **—Ä–∞–±–æ—Ç–∞—é—Ç** —Å —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π. –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.

–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–§–∞–∑–∞ 1).

